# Nano-Mamba3 Hybrid Configuration
# Combines Mamba-3 SSM layers with MLA+MoE attention layers
# Best of both worlds: efficient recurrence + strong attention for long-range deps
#
# Architecture: 6 Mamba3 + 2 MLA+MoE (alternating pattern)
# ~60M parameters

model:
  hidden_size: 384
  num_layers: 8
  num_heads: 6

  # MLA (Multi-Head Latent Attention) for attention layers (3, 7)
  kv_latent_dim: 192 # KV compression dimension
  q_latent_dim: 192 # Q compression dimension
  d_rope: 16 # Decoupled RoPE dimension

  # MoE (Mixture of Experts) for attention layers
  num_experts: 4 # Number of experts
  experts_per_tok: 2 # Top-k routing
  intermediate_size: 1536 # Expert FFN intermediate size

  # Hybrid architecture: Mamba3 for layers 0,1,2,4,5,6; MLA+MoE for layers 3,7
  mamba_layers: [0, 1, 2, 4, 5, 6]

  # Mamba3 Configuration (for layers 0,1,2,4,5,6)
  mamba3_enabled: true # Enable Mamba-3 innovations
  mamba3_complex_rope: true # Enable complex-valued RoPE for state tracking
  mamba3_mimo_rank: 0 # SISO mode (use 4 for MIMO for better GPU utilization)
  mamba3_use_conv: false # Trapezoidal discretization replaces conv

  # Mamba2 base parameters (shared with Mamba3)
  # CONSTRAINT: hidden_size * mamba2_expand == mamba2_num_heads * mamba2_head_dim
  # Validation: 384 * 2 = 768 == 48 * 16 âœ“
  mamba2_num_heads: 48 # Number of SSM heads
  mamba2_head_dim: 16 # Dimension per head
  mamba2_state_size: 64 # SSM state dimension N
  mamba2_chunk_size: 64 # Chunk size for SSD
  mamba2_n_groups: 1 # Groups for efficient computation
  mamba2_conv_kernel: 4 # Conv1D kernel size (if use_conv enabled)
  mamba2_expand: 2 # Expansion factor (intermediate = 768)

  vocab_size: 128354 # Llama 3 + splintr agent tokens
  max_seq_len: 512
  rope_theta: 10000.0

trainer:
  target_device: gpu
  learning_rate: 0.0003
  max_steps: 0
  num_epochs: 2
  batch_size: 4
  gradient_accumulation: 1
  checkpoint_dir: "./checkpoints/nano-mamba3-hybrid"
  log_interval: 10
  save_interval: 500
  load_balance_alpha: 0.01 # MoE load balance loss weight

  # GRPO config (not used for pretraining)
  group_size: 4
  beta_kl: 0.04
